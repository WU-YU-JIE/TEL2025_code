# scan_folder.py
# -*- coding: utf-8 -*-
import os, sys, csv, time, json, base64, hashlib
from collections import defaultdict

DEFAULT_TARGET = r"C:\Users\user\OneDrive - ä¸­åŸå¤§å­¸\æ¡Œé¢\å¤§å­¸ä½œå“é›†\æ±äº¬å¨åŠ›\TEL2025_MainCode"

# ç•¥éçš„è³‡æ–™å¤¾
SKIP_DIRS = {
    ".git","__pycache__","node_modules",".venv","venv","env",
    "build","dist",".idea",".vscode",".pytest_cache",".mypy_cache",".cache"
}

# ç•¶ä½œæ–‡å­—æª”è™•ç†ï¼ˆå°å¯«å‰¯æª”åï¼‰
TEXT_EXTS = {
    ".py",".c",".cpp",".h",".hpp",".java",".js",".ts",".sh",".bat",".ps1",".ino",
    ".go",".rb",".php",".swift",".kt",".m",".cs",".json",".yml",".yaml",".toml",
    ".ini",".md",".txt",".csv",".xml",".html",".css"
}

# å…§å®¹è¼¸å‡ºæ§åˆ¶
MAX_TEXT_BYTES = 512 * 1024      # å–®ä¸€æ–‡å­—æª”æœ€å¤šè®€ 512 KB å…§å®¹å–”
MAX_BINARY_BYTES = 0             # äºŒé€²ä½æª”æ˜¯å¦è¼¸å‡º base64ï¼ˆ0=ä¸è¼¸å‡ºï¼Œåªè¨˜éŒ„é›œæ¹Šèˆ‡ä¸­ç¹¼è³‡æ–™ï¼‰
ENCODING = "utf-8"               # æ–‡å­—è®€å–ç·¨ç¢¼ï¼ˆå¤±æ•—å‰‡ç”¨ errors='replace'ï¼‰

def is_probably_binary(path, ext) -> bool:
    if ext.lower() in TEXT_EXTS:
        return False
    try:
        with open(path, "rb") as f:
            head = f.read(4096)
        # æœ‰ NUL æˆ–å¤ªå¤šéå¯åˆ—å°æ§åˆ¶å­— â†’ è¦–ç‚ºäºŒé€²ä½
        if b"\x00" in head:
            return True
        nontext = sum(1 for b in head if b < 9 or (13 < b < 32) or b == 127)
        return nontext / max(1, len(head)) > 0.30
    except Exception:
        return True

def human_size(n: int) -> str:
    units = ["B","KB","MB","GB","TB"]
    i, f = 0, float(n)
    while f >= 1024 and i < len(units)-1:
        f /= 1024; i += 1
    return f"{f:.2f} {units[i]}"

def sha256_file(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def gen_tree(root: str, skip_dirs=SKIP_DIRS, max_files_per_dir=200) -> str:
    lines = []
    root = os.path.abspath(root)
    for current_root, dirs, files in os.walk(root):
        rel = os.path.relpath(current_root, root)
        depth = 0 if rel == "." else rel.count(os.sep) + 1
        indent = "    " * depth
        lines.append((os.path.basename(root) if rel == "." else f"{'    '*(depth-1)}â””â”€ {os.path.basename(current_root)}"))
        dirs[:] = [d for d in dirs if d not in skip_dirs]
        shown = 0
        for name in sorted(files):
            if shown >= max_files_per_dir:
                lines.append(f"{indent}â””â”€ ... ({len(files)-shown} more files)")
                break
            lines.append(f"{indent}â””â”€ {name}")
            shown += 1
    return "\n".join(lines)

def main():
    target = sys.argv[1] if len(sys.argv) >= 2 else DEFAULT_TARGET
    target = os.path.abspath(os.path.expanduser(target))
    if not os.path.isdir(target):
        print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾ï¼š{target}")
        sys.exit(1)

    out_csv  = os.path.join(target, "files_report.csv")
    out_tree = os.path.join(target, "files_tree.txt")
    out_jsonl = os.path.join(target, "files_content.jsonl")
    out_alltxt = os.path.join(target, "ALL_TEXTS.txt")

    by_ext_count = defaultdict(int)
    by_ext_size = defaultdict(int)
    total_files = total_bytes = 0

    # å…ˆå¯« CSVï¼ˆä¸­ç¹¼è³‡æ–™ï¼‰
    with open(out_csv, "w", newline="", encoding="utf-8-sig") as fcsv, \
         open(out_jsonl, "w", encoding="utf-8") as fjsonl, \
         open(out_alltxt, "w", encoding="utf-8", errors="replace") as falltxt:

        writer = csv.writer(fcsv)
        writer.writerow([
            "relative_path", "extension", "size_bytes", "size_human",
            "modified_time", "is_text", "line_count", "sha256"
        ])

        for root_dir, dirs, files in os.walk(target):
            dirs[:] = [d for d in dirs if d not in SKIP_DIRS]
            for fname in files:
                fpath = os.path.join(root_dir, fname)
                rel = os.path.relpath(fpath, target)
                ext = os.path.splitext(fname)[1].lower()

                try:
                    stat = os.stat(fpath)
                except OSError:
                    continue

                size = stat.st_size
                mtime = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stat.st_mtime))
                is_bin = is_probably_binary(fpath, ext)
                sha256 = sha256_file(fpath)

                line_count = ""
                text_preview = ""
                encoding_used = None

                if not is_bin:
                    # è®€éƒ¨åˆ†æ–‡å­—å…§å®¹ï¼ˆé™åˆ¶å¤§å°ï¼‰
                    try:
                        with open(fpath, "r", encoding=ENCODING, errors="replace") as fr:
                            content = fr.read(min(MAX_TEXT_BYTES, size))
                            encoding_used = ENCODING
                            text_preview = content
                            line_count = content.count("\n") + (1 if content and not content.endswith("\n") else 0)
                    except Exception:
                        pass

                    # å¯«å…¥ ALL_TEXTS åŒ¯ç¸½
                    falltxt.write("\n" + "="*80 + "\n")
                    falltxt.write(f"FILE: {rel}\n")
                    falltxt.write("="*80 + "\n")
                    if text_preview:
                        falltxt.write(text_preview)
                        if size > MAX_TEXT_BYTES:
                            falltxt.write("\n[... TRUNCATED ...]\n")
                    else:
                        falltxt.write("[[EMPTY OR UNREADABLE TEXT]]\n")

                    # JSONLï¼šæ–‡å­—
                    json.dump({
                        "path": rel,
                        "extension": ext or "(noext)",
                        "size_bytes": size,
                        "modified_time": mtime,
                        "sha256": sha256,
                        "is_text": True,
                        "encoding": encoding_used or "utf-8",
                        "truncated": bool(size > MAX_TEXT_BYTES),
                        "content": text_preview
                    }, fjsonl, ensure_ascii=False)
                    fjsonl.write("\n")

                else:
                    # JSONLï¼šäºŒé€²ä½ï¼ˆé è¨­ä¸å«å…§å®¹ï¼Œåªè¨˜é›œæ¹Šèˆ‡ä¸­ç¹¼è³‡æ–™ï¼›éœ€è¦å¯æ”¹ MAX_BINARY_BYTESï¼‰
                    b64 = None
                    if MAX_BINARY_BYTES > 0:
                        try:
                            with open(fpath, "rb") as fb:
                                raw = fb.read(min(MAX_BINARY_BYTES, size))
                            b64 = base64.b64encode(raw).decode("ascii")
                        except Exception:
                            b64 = None

                    json.dump({
                        "path": rel,
                        "extension": ext or "(noext)",
                        "size_bytes": size,
                        "modified_time": mtime,
                        "sha256": sha256,
                        "is_text": False,
                        "base64_bytes": b64,
                        "truncated": bool(size > MAX_BINARY_BYTES) if MAX_BINARY_BYTES > 0 else None
                    }, fjsonl)
                    fjsonl.write("\n")

                # CSVï¼ˆä¸­ç¹¼è³‡æ–™ä¸€è¦½ï¼‰
                writer.writerow([
                    rel, ext or "(noext)", size, human_size(size), mtime,
                    int(not is_bin), line_count, sha256
                ])

                by_ext_count[ext] += 1
                by_ext_size[ext] += size
                total_files += 1
                total_bytes += size

    # ç”¢ç”Ÿæ¨¹ç‹€
    with open(out_tree, "w", encoding="utf-8") as ftree:
        ftree.write(gen_tree(target))

    # æ‘˜è¦
    print("ğŸ“ æƒæå®Œæˆï¼š", target)
    print(f"   æª”æ¡ˆç¸½æ•¸ï¼š{total_files}")
    print(f"   ç¸½å¤§å°  ï¼š{human_size(total_bytes)}")
    print("   ä¾å‰¯æª”åçµ±è¨ˆï¼ˆå‰ 15 åï¼‰ï¼š")
    top = sorted(by_ext_count.items(), key=lambda kv: by_ext_size[kv[0]], reverse=True)[:15]
    for ext, cnt in top:
        print(f"   {ext or '(noext)':>8}  {cnt:>6} æª”  {human_size(by_ext_size[ext]):>10}")

    print(f"\nâœ… å·²è¼¸å‡ºï¼š\n  - {out_csv}\n  - {out_tree}\n  - {out_jsonl}\n  - {out_alltxt}")
    print("\nâš™ï¸ åƒæ•¸ï¼šMAX_TEXT_BYTES={}, MAX_BINARY_BYTES={}ï¼ˆå¯åœ¨æª”é ­èª¿æ•´ï¼‰".format(MAX_TEXT_BYTES, MAX_BINARY_BYTES))
    print("ğŸ’¡ JSONL å¯è¢« jq / pandas / BigQuery / Elasticsearch ç­‰ç›´æ¥ä½¿ç”¨ï¼›ALL_TEXTS.txt æ–¹ä¾¿äººå·¥æª¢è¦–ã€‚")

if __name__ == "__main__":
    main()
